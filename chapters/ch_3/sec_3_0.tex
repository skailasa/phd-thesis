\section{Distributed Octrees}

In this section we describe our software for the creation of octrees for the kiFMM in $\mathbb{R}^3$, Bempp-Tree\footnote{https://github.com/bempp/bempp-rs/tree/main/tree}. We've re-implemented optimal algorithms for the `bottom-up' construction of trees in parallel, whereby points distributed across each node are assembled into a parallel tree partitioned and load-balanced across a distributed system \cite{sundar2008bottom}. We demonstrate that on a single-node, our tree software performs well in comparison to leading single-node FMM codes \cite{wang2021exafmm}, where parallel experiments are omitted due to time constraints. In addition to good scaling, we leverage the power of Rust's Traits to write software that generalises across single-node and multi-node trees, allowing for users to implement single/multi-node fast algorithms with relative ease. We begin by describing in detail our method for constructing trees, following the discussion in \cite{sundar2008bottom} though we use an updated communication scheme in order to achieve a 2:1 balance based on more recent work in \cite{sundar2013hyksort}. We conclude with a short discussion on the communication intensive phases of the FMM in a distributed-memory FMM implementation, and how these can be addressed, as the next stage of this research project centers on the construction of a distributed memory kiFMM.

We make use of a standard space-filling curve known as a Morton encoding \cite{sundar2008bottom} to encode the boxes of an octree as described in Figure \ref{fig:chpt:3:sec:0:morton}. This approach encodes spatial locality in that boxes encoded in sorted Morton order correspond to a post-order traversal of the corresponding octree. Once encoded the Morton encoding for a given box is referred to as a \textit{Morton key}. The main terminology which we will make use of with regards to trees created using Morton encodings are summarised Table \ref{table:chpt:3:sec:0:morton}.

Historically implementations of distributed memory octrees and quadtrees were `pointer based' \cite{tu2005scalable}. Starting with a random subset of points at each processor, each node would recursively refine its local octree until it arrived at level of recursion limited by a user defined constraint on the maximum number of particles in a leaf box. However this is complicated by the need to communicate and synchronise ancestor relationships across nodes. Furthermore as the point distribution is not generally known apriori, with each process controlling a local tree involving a subset of the initial points, reconciling local trees necessitates a global parallel merge. Overlapping boxes would result in the user defined constraint on the maximum number of particles per box being violated, necessitating further refinement. Further communication would then be required if a user requires a 2:1 balance. Finally, load-balancing to ensure each node has an approximately uniform weighting in terms of particles would be computed as a post processing step.

The significant complexities in implementing such trees has lead to softwares that rely on a `bottom up' approach, as opposed to a `top down' strategy as described above \cite{sampath2008dendro,BursteddeWilcoxGhattas11}. The basic strategies implemented here is to form a Morton encoding for points at each processor and use parallel sorts to ensure that each node contains a non-overlapping subset of the global tree. The Morton keys are kept only at the leaf level, with ancestor keys inferred to exist. These trees are therefore referred to as \textit{linear} trees, and have been shown to scale to trees on tens of thousands of processors with billions of octants \cite{BursteddeWilcoxGhattas11}.

Beginning with point data distributed across $n_p$ nodes in a distributed memory system, our strategy for creating uniform and adaptive octrees is as follows,

\begin{enumerate}
    \item Generate a Morton key for each point at the leaf level corresponding to a user defined octree depth (uniform trees), or based on a critical value for the maximum number of points per leaf box (adaptive trees).
    \item Perform a parallel sort of these leaf octants, such that processors contain non-overlapping subsets of the global tree.
    \item \textit{Linearise} the leaves on each compute node, i.e. remove overlaps, favouring smaller leaves over larger ones. We use Algorithm \ref{alg:app:morton:linearise_octree} in Appendix \ref{app:morton} to do so.
    \item `Complete' the space defined by the largest and smallest leaf on each compute node. This amounts to finding the minimal octree that covers the region specified by the Morton keys at each process. This is described by Algorithm \ref{alg:app:morton:complete_region} in Appendix \ref{app:morton}. The coarsest boxes of this `complete' tree are referred to as `seeds'.
    \item Construct a `coarse block tree' by completing the space between seeds on each processor. The nodes of this tree are referred to as blocks, and help us to estimate the load balance of each compute node by calculating the number of original Morton keys they contain. This gives us a coarse distributed, complete, linear octree that is based on the underlying data distribution and contains load information.
    \item Point data is communicated to each compute nodes containing its associated block, and the blocks are refined based on the level of recursion defined by the user, which depends again on whether uniform or adaptive trees are requred. Providing the final, unbalanced, tree in the case of adaptive trees.
    \item For adaptive trees a 2:1 balance is computed on each subtree on each compute node using Algorithm \ref{alg:app:morton:balance_octree} in Appendix \ref{app:morton}. The locally balanced leaf octants can be sorted in parallel again, and locally linearised as in Step 3, to remove overlaps. Thus we are left with a globally balanced, linear, octree based on the original data distribution.
\end{enumerate}

This algorithm is summarised in Algorithm \ref{alg:app:morton:point2octree} in Appendix \ref{app:morton}. The complexity of this process is bounded by the parallel sorts, which for randomly distributed point data, run in $O(N_{\text{leaf}} \log (N_{\text{leaf}}))$ work and $O(\frac{N_{\text{leaf}}}{n_p} \log(\frac{N_{\text{leaf}}}{n_p}) + n_p \log (n_p))$ time where $N_{\text{leaf}}$ is the number of leaves in the final octree, and $n_p$ is the number of MPI processes \cite{sundar2013hyksort}. This makes the efficiency of the parallel sort a bottleneck in tree construction.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/ch_3/morton.pdf}
    \caption{How to Morton encode a node in an octree. The node is described by an ‘anchor’ vertex, and its associated coordinate.}
    \label{fig:chpt:3:sec:0:morton}
\end{figure}


\begin{table}[h!]
\centering
\begin{tabular}{||c c||}
    \hline
    Relation & Definition \\ [0.5ex]
    \hline\hline
    Siblings($N$) & The seven (octree) or three (quadtree) Keys \\
        & that share a parent with $N$  \\
    Neighbours($N$) & Keys that share a face, edge, or vertex with $N$  \\
    Parent($N$) & The parent key of $N$  \\
    Children($N$) & The eight (octree) or four (quadtree) children of $N$  \\
    Descendants($N$) & All descendants of $N$  \\
    Ancestors($N$) & All ancestors of $N$  \\
    FinestAncestors($N$, $M$) & Finest shared ancestor of $N$ and $M$.  \\
    FirstChild($N$) & The `first', in Morton order, child of $N$  \\
    LastChild($N$) & The `last', in Morton order, child of $N$  \\
    DeepestFirstDescendant($N$) & The `first', in Morton order,  \\
        &  descendant of $N$ at the leaf level \\
    DeepestLastDescendant($N$) & The `last', in Morton order, \\
        & descendant of $N$ at the leaf level \\

    \hline
\end{tabular}
\caption{Definitions of Morton key relations for a given Morton key $N$.}
\label{table:chpt:3:sec:0:morton}
\end{table}

Designing and implementing an efficient sorting algorithm that can scale to thousands of cores is difficult since it requires irregular data access, communication, and equal load-balance. As first presented by Sundar et al, parallel sorts in the creation of octrees were implemented using a variant of Sample Sort \cite{sundar2008bottom}. Briefly, this approach samples elements at each processor to create a set of $n_p - 1 $ ordered `splitters', which are shared across all processors and define a set of $n_p$ buckets. This is followed by a global all-to-all communication call over all $n_p$ processors to assign elements to their corresponding bucket. Finally, a local sort is performed at each bucket to produce a globally sorted array. SampleSort is well understood. However, its performance is quite sensitive to the selection of splitters, which can result in load imbalance. Most importantly, the all-to-all key redistribution scales linearly with the number of tasks and can congest the network. As a result sample sort may scale sub-optimally, especially when the communication volume approaches the available hardware limits \cite{sundar2013hyksort}.

An alternative approach is provided by HykSort \cite{sundar2013hyksort}, which is a generalisation of Quicksort over a hypercube \cite{wagar1987hyperquicksort} from 2-way splits to $k$-way splits, with the addition of an optimised algorithm to select splitters. Quicksort, Hyksort and Sample Sort are compared in figure (\ref{fig:chpt:3:sec:0:hyksort}). Instead of splitting the global array into $n_p$ buckets, Hyksort splits it into $k < n_p$, and recursively sorts for each bucket. We notice that at each recursion step, each task communicates with just $k$ other tasks. Indeed, for $k=2$ we recover Quicksort over a hypercube. Both Hyksort, and the parallel splitter selection algorithm are provided in Appendix \ref{app:hyksort}, alongside complexity comparisons between Hyksort and Sample Sort. We note that Hyksort has a lower asymptotic complexity, with no terms that scale linearly in number of MPI processes, $n_p$, unlike Sample Sort.

\begin{figure}
    \centerline{\includegraphics[width=0.7\linewidth]{images/ch_3/hyksort.pdf}}
    \caption{Communication pattern of Hyksort algorithm compared to a parallel Sample Sort, as well as a Quicksort over a hypercube, adapted from \cite{sundar2013hyksort}. We see that HykSort results in a lower communication overhead than Sample Sort.}
    \label{fig:chpt:3:sec:0:hyksort}
\end{figure}


Figure \ref{fig:chpt:3:sec:0:single_node_scaling} shows the scaling of our software, `Bempp-Tree', on a single node as time limitations inhibited multi-node experiments. In the left figure compare the performance of linear trees constructed using our software with the pointer-based trees constructed using the functionality of ExaFMM-t the leading single-node implementation of the kiFMM. We limit this comparison to uniform trees as they don't offer adaptive functionality. In the right figure we show weak scaling for fixed 1e6 points per MPI process for adaptive and uniform trees. We observe that there is a constant overhead in constructing adaptive trees, this comes from having to recursively refine blocks from the block tree by counting how many points they contain. The significant jump when using 8 MPI processes is likely due to the number of MPI processes exceeding the number of cores available on the Intel i7-9750 processor used for the experiments. Our performance in the single-node case is excellent, where we're able to generate an octree with 8e6 points, with a depth of 5, in approximately 13.5s. A marked improvement over ExaFMM-T, and demonstrative of the power of a bottom up approach in comparison to a pointer based approach used by that software. Multi-node experiments are required to assess the performance of our library in comparison to other state of the art tree libraries \cite{sampath2008dendro,BursteddeWilcoxGhattas11}. However, as our approach closely follows theirs we don't expect performance to be significantly different.

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/ch_3/single_node_scaling.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/ch_3/weak_scaling_graph.png}
  \end{minipage}
\caption{The left figure shows the runtime of creating uniform trees on a single node in Bempp-Tree, in comparison to ExaFMM-T. The right figure shows weak scaling (over cores on a single node) of creating uniform and adaptive trees with Bempp-Tree, where each MPI process is given 1e6 points. The uniform trees are partitioned to a depth of 5, the adaptive trees have at most 150 particles in their leaf boxes. Experiments were taken on an Intel i7-9750H 6 core processor.}
\label{fig:chpt:3:sec:0:single_node_scaling}
\end{figure}

\subsection{Exposing Parallelism in A Distributed Memory FMM}

In the context of FMMs generating a distributed tree constitutes the first communication intensive phase. Further bottlenecks occur in the evaluation of the operators, most significantly in the evaluation of $T^{M2M}$ and $T^{M2L}$. To understand where these operators lead to communication bottlenecks, and how these can be overcome in practical implementations, we now introduce terminology relevant to distributing the FMM on parallel machines. We build upon the discussion first presented in Section 3 of \cite{Lashuk2012}, and use the same terminology.

Given a partition of a global tree $T$, as described above, such that each compute node $k$ contains a disjoint subset of leaves arranged in Morton order, $T_k$ we define a \textit{Locally Essential Tree} (LET) for compute node $k$ as the union of interaction lists for all its owned leaves, as well as their ancestors,

\begin{flalign}
    \text{LET}(k) := \cup_{N \in T_k \cup \text{Ancestors}(T_k)} \text{InteractionList}(N)
    \label{eq:chpt:3:sec:0:let}
\end{flalign}

We denote the region controlled by MPI process $k$ as $\Omega_k$, from the properties of Morton keys (see app. \ref{app:morton}) this is defined by the smallest and largest Morton key they hold. Using an MPI\_AllGather collective, we exchange information about the bounds of each MPI process globally. Following this, ancestors are added to each $T_k$ for the leaf nodes they control. In order to communicate `ghost octants', i.e. octants which are relied upon by the translation operators acting upon each $T_k$ but are not held locally, we introduce the concept of `Contributor' and `User' compute nodes,

\begin{enumerate}
    \item Contributor compute nodes for an octant $N \in T$ are
    \[
        \mathcal{P}_c(N) := k \in 1...p : N \text{ overlaps with } \Omega_k
    \]
    \item User compute nodes of an octant $N \in T$ are
    \[
        \mathcal{P}_u(N) := k \in 1...p : \text{ Parent($N$) or Neighbours ($N$) overlaps with } \Omega_k
    \]
\end{enumerate}

We denote the set of octants which compute node $k$ contributes and which process $k'$ uses as $I_{k k'}$. These are communicated and inserted into each compute node's local tree in order to complete the construction of its LET. Consider the potential generated by the sources enclosed by some octant $N$. In order to evaluate the potential outside the volume covered by $\text{Neighbours}(\text{Parent})(N)$ one doesn't need the multipole expansions, or sources, associated with $N$ as an ancestor of $N$ would be used instead. This ensures the correctness of this approach to constructing LETs.

Once an LET has been constructed, and particle information has been exchanged to user processors, the $T^{P2M}$ and $P2P$ operators don't require further communication. Lashuk et. al reason that this allows for the potential pipelining of $T^{P2M}$, $T^{M2M}$ and $T^{M2L}$, though they have not experimented with this. They identify the $T^{M2M}$ and $T^{M2L}$ operators as those requiring intensive communication phases during FMM evaluation. Beginning with $T^{M2M}$, the communication and calculation can be seen to progress in three steps,

\begin{enumerate}
    \item Firstly, source charges which are required for $P2P$ ar communicated with user MPI processes. As these will be relatively `local' communications within the context of a distributed memory machine, they use non-blocking point-to-point `Isend' functions for this step.
    \item Secondly, the multipole coefficients are summed for the octants on the coarser level.
    \item Thirdly, the multipole coefficients are communicated to user octants, as each MPI rank has only a partial sum for the octants at the parent level of each octant.
\end{enumerate}

During the downward pass Lashuk et. al.'s scheme doesn't require any further communication as each MPI process has all the multipole data, as well as point data, to compute the FMM for its locally held particles in parallel. The complexity of the communication of this scheme is given as $O(\sqrt{n_p}(N/n_p)^{2/3})$ where $n_p$ is the total number of MPI processes and $N$ is the number of particles for octrees created in $\mathbb{R}^3$. This complexity applies as upper bound for non-uniform point distributions.

Ibeid et. al \cite{Ibeid2016} propose an alternative communication scheme that achieves a tighter complexity bound of $O(\log(n_p) + (N/n_p)^{2/3})$. They rely on an alternative abstraction for handling the tree, by splitting it into a `global tree' and `local tree'. Their scheme is illustrated in Figure \ref{fig:chpt:3:sec:0:ibeid}, which describes a uniformly refined tree created for uniform point distributions\footnote{This is proved for uniform point distributions, and a uniformly refined octree in \cite{Ibeid2016}, but is also demonstrated to apply to non-uniform distributions.}, which we've adapted from Figure 3 in \cite{Ibeid2016}. Here a global tree is constructed such that the \textit{root node} of the tree on each MPI process is \textit{leaf node} of the global tree. The depth of the global tree, defined by $L_{\text{global}}$ in fig \ref{fig:chpt:3:sec:0:ibeid}, is thus defined by the number of MPI processes and is therefore $O(\log(n_p))$. The depth of the local tree is defined by the particles belonging to the MPI process and is seen to be $O(\log(N/n_p))$. By splitting the tree in this way, the communications during the $T^{M2M}$ and $T^{M2L}$ operators can be split into `global' and `local' phases. At the leaf nodes of the global tree, which starts at level $L_{\text{global}}$, after $T^{M2M}$ is performed, 8 neighbouring MPI processes contain the same information for boxes at $L_{\text{global}}-1$. Therefore, only one of them has to perform communications for the $T^{M2M}$ operation for the next level, proceeding recursively until the root of the global tree is reached. This corresponds to an 8-fold redundancy in the communication scheme proposed by Lashuk et. al. Using this, the number of communications at coarser global tree levels for each box stays constant at 7, and the communication complexity of the $T^{M2M}$ operator is proportional to the depth of the global tree $O(\log(n_p))$. The same redundancy can be used in reverse during the downward pass to communicate the multipole coefficients during $T^{M2L}$ on the global tree, which has the same complexity. We note however that in practice communications will occur between increasingly distant nodes at coarser levels of the tree, meaning that specific network topologies or interconnects would have an impact on practical performance.

Communications are still required during the downward pass of the local trees for computing $T^{M2L}$ and the $P2P$ operators. However, the communications are `local' in the sense that the same set of MPI processes will communicate with each other due to their overlapping LETs. This limits the number of processes to communicate with to $O(1)$ during these steps, as opposed to $O(n_p)$. However, as the number of boxes increases exponentially at finer tree levels, the amount of information being sent grows with increasing tree depth. This amounts to an increasing ratio of the surface to volume for finer boxes. The same is true for communicating the source point data for the $P2P$ operator. They find the number of boxes to be sent for $T^{M2L}$ at the $i^{th}$ level of the local tree to be $(2^i + 4)^3 - 8^i$ and $(2^i + 2)^3-8^i$ respectively. This can be seen by the fact that the interaction list for $T^{M2L}$ requires the communication of a halo that is 2 boxes wide, which adds 4 boxes per dimension, and similarly the $P2P$ requires the communication of a halo that is 1 box wide, which adds 2 boxes per dimension, and there are $8^i$ boxes at the $i^{th}$ level. Summing up the powers of four up to the depth of the local tree gives,

\begin{flalign}
    \sum_{i=0}^{\log_8(N/n_p)} 4^i
\end{flalign}

taking the leading order term for this geometric series,

\begin{flalign*}
    \sum_{i=0}^{\log_8(N/n_p)} 4^i &= O(4^{\frac{\log_4(N/n_p)}{\log_4(8)}}) \\
    &= O(4^{2/3\log_4(N/n_p)}) \\
    &=O((N/n_p)^{2/3})
\end{flalign*}

Giving a communication complexity of $O((N/n_p)^{2/3})$ for the local tree communications, and proving the bound provided by Ibeid et. al.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/ch_3/ibeid.pdf}
    \caption{The tree structure shown here is a binary tree chosen for clarity by Ibeid et. al, this would be an octree in $\mathbb{R}^3$.}
    \label{fig:chpt:3:sec:0:ibeid}
\end{figure}

The near term priorities for our tree implementations are to implement the optimised communication scheme of Ibeid et. al., and compare its practical performance with the scheme provided by Lashuk et. al. Real architectures may offer discrepancies in performance from the that expected from complexity analysis as Ibeid et. al's approach results in communication between physically distant MPI processes and the scheme of Lashuk et. al. benefits from being simpler to implement, with low-effort opportunities for further optimisations through operation pipelining and has been demonstrated to scale well to thousands of MPI processes, with billions of points.
