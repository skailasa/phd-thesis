Octrees are a foundational data structure for fast algorithms in $\mathbb{R}^3$. Writing software for octrees that can be distributed across parallel computing systems is challenging, demonstrated by the limited availability of open-source software \cite{BursteddeWilcoxGhattas11,fernando2020github}. This is inspite of their ubiquity in scientific computing applications from adaptive finite-element methods to many-body algorithms \cite{sundar2008bottom}. We present Rusty Tree, an implementation of MPI-distributed octrees with a complete Python interface. Rusty Tree is a proof of concept for Rust as a tool for high-performance computational science. We document our experience in developing Rusty Tree, and use it as a tool to explore the current landscape of scientific computing with Rust. From multithreading, and developing a Python interface, to distributing applications with MPI, writing Rusty Tree involved using much of Rust's scientific computing ecosystem. 

\subsection*{Parallel Octrees}

As mentioned in section \ref{sec:2_2}, there are multiple representations of octrees. We choose a linear octree representation, in which we discard interior nodes and store a list of leaves, as in our previous work PyExaFMM \cite{kailasa2022pyexafmm}. These lie in contrast to pointer based octrees, which have a relatively higher storage cost, in addition to a synchronisation and communication overhead for parallel implementations that must keep track of pointers across nodes \cite{tu2005scalable}. 

Octrees are usually constructed `top down'. A user specifies a threshold for the maximum number of particles in a leaf node, $n_{\text{crit}}$, and a unit cube encapsulating the problem domain is refined until this is satisfied. After refinement, one can optionally `balance' adjacent leaf nodes such that adjacent nodes obey a relative size constraint. However, translating this logic to a parallel setting is challenging. Constructing octrees `top down' involve a significant data communication that come from having to synchronise nodes across processors during refinement. For example,

\begin{itemize}
    \item Balancing requires updating a chain of relationships, that can `ripple' across processors.
    \item As the data distribution is not known a priori, load balancing must be done as a post-processing step.
\end{itemize}

Sundar et al \cite{sundar2008bottom}, therefore introduce `bottom up' tree construction. The strategy relies on a `space filling curve' that passes through each octant in an octree, for which they choose a Morton encoding. Morton encoding have excellent properties for spatial decompositions. Once sorted, adjacent Morton keys encode spatial locality. Indeed traversing a sorted list of Morton keys from smallest to greatest is equivalent to a level by level traversal of an octree, starting at its root node. Similarly, a reverse order traversal is equivalent to a level by level traversal of an octree starting at its leaf level. We show how to calculate the Morton encoded key for a given octant in figure (\ref{fig:sec_2_4:morton}). This process is accelerated in RustyTree using the Rayon library for parallel multithreading, see box [TODO: RAYON AND MORTON ENCODING BOX].

Given a set of evenly distributed point data across $N$ processors, Sundar et. al's strategy is to:

\begin{enumerate}
    \item Generate a Morton key for each point corresponding to a user defined octree depth.
    \item Perform a parallel sort of these leaf octants using MPI, such that processors with adjacent MPI ranks have adjacent data.
    \item \textit{Linearise} leaves on each processor, such that their leaves do not overlap, in a manner that favours larger leaves over smaller ones, as in algorithm [TODO: LINEARISATION ALGORITHM].
    \item \textit{Complete} the space defined by the leaves on each processor, to find the domain occupied by their leaves, see figure [TODO: COMPLETE REGION].
    \item Find the minimum spanning set of nodes that occupies the space defined by the leaves in a given processor, and define the coarsest nodes of this set as \textit{seeds}, as in algorithm [TODO: seed finding algorithm].
    \item Construct a \textit{block tree}, by completing the space between the seeds with the minimum spanning set of nodes. The nodes of this block tree are referred to as \textit{blocks}. This gives us a distributed coarse complete linear octree that is based on the underlying data distribution, and is described in algorithm [TODO: block tree algorithm].
    \item The load of each block is calculated by using the number of original octants calculated in step (1). The blocks are then redistributed so that each processor contains an approximately equal load, using algorithm [TODO: load balancing algorithm] - ALSO HAVE FORGOTTON TO implement!.
    \item Point data is communicated to the processor containing its associated block, and the blocks are refined based on $n_{\text{crit}}$, providing the final, load balanced, linear octree tree.
    \item Optionally, the tree is balanced. This can be done for the local subtree on each processor using algorithm [TODO: BALANCING ALGORITHM]. The locally balanced trees can then be sorted in parallel again, and locally linearized as in step (3).
\end{enumerate}

The complexity of this process is bounded by the parallel sorts, which for randomly distributed point data, run in $O(N_{\text{leaf}} \log (N_{\text{leaf}}))$ work and $O(\frac{N_{\text{leaf}}}{n_p} \log(\frac{N_{\text{leaf}}}{n_p}) + n_p \log (n_p))$ time where $N_{\text{leaf}}$ is the number of leaves in the final octree, and $n_p$ is the number of processors (see appendix \ref{app:a_2_complexity_tree_construction} for a derivation). Making the parallel sort's efficiency a bottleneck for tree construction.

\begin{figure}
    \centerline{\includegraphics[width=\linewidth]{ch_2/morton.pdf}}
    \caption{How to Morton encode a node in an octree.}
    \label{fig:sec_2_4:morton}
\end{figure}

\subsection*{Efficient Parallel Sorts}

Designing and implementing an efficient sorting algorithm that can scale to thousands of cores is difficult since it requires irregular data access, communication, and load-balance. In the original paper \cite{sundar2008bottom}, Sundar et. al use a variant of sample sort. Briefly, this approach samples elements at each processor to create a set of $n_p - 1 $ ordered `splitters', which are shared across all processors and define a set of $n_p$ buckets. This is followed by a global all-to-all communication call over all $n_p$ processors to assign elements to their corresponding bucket. Finally, a local sort is performed at each bucket to produce a globally sorted array. SampleSort is well understood. However, its performance is quite sensitive to the selection of splitters, which can result in load imbalance. Most important, the all-to-all key redistribution scales linearly with the number of tasks and can congest the network. As a result SampleSort may scale suboptimally, especially when the communication volume approaches the available hardware limits \cite{sundar2013hyksort}. Indeed, when tested at the largest experimental values involving $~1e9$ particles, and $n_p \geq 10^2$, the library defined \pythoninline{MPI_Alltoallv()} is not able to cope with the volume of data being sent across the network, and results in a segfault. We compare HykSort to a Rust implementation of Sample Sort in figure [TODO: Comparison of Hyksort and Samplesort (naive)], noting that HykSort provides a [TODO: performance improvement], and scales to [TODO: Max scale comparison].

An alternative approach is provided by Sundar et. al's `HykSort' \cite{sundar2013hyksort}. Here they reduce the communication overhead with the following improvement to sample sort. Instead of splitting the global array into $n_p$ buckets, they split into $k < n_p$, and recursively sort for each bucket. This situation is sketched in figure [TODO: Figure of sample sort]. We notice that at each recursion step, each task communicates with just $k$ other tasks. Indeed, for $k=2$ we recover Quicksort over a hypercube. 

Sundar et. al. also provide an algorithm to optimally select splitters, see algorithm [TODO: splitter algorithm], and a SIMD optimised local merge-sort, see algorithm [TODO: merge sort algorithm]. Upon comparison between their implementation to Rust's inbuilt sorting function in figure [TODO: Comparison of Rust with usort for local sort] we decided to use Rust's sort method in our implementation.

\subsection*{Rusty Tree}

Rusty Tree can consumed as either a Rust crate or as a Python package distributed via Conda, with uniform installation steps on different platforms. We have so far tested Rusty Tree on MacOS, RedHat [TODO: Lookup what Timo's machine uses] and [TODO: Lookup what Myriad uses]. Rusty Tree contains a full test suite, and documentation, built using Cargo, and can be found at \url{https://github.com/rusty-fast-solvers/rusty-tree}.

Example usage in Python mirrors the Rust interface [TODO: Python Vs Rust experiment]. Users are able use point data generated at runtime, or provide a HDF5 file for Rusty Tree to consume. Rusty Tree is also capable of outputting a VTK file for visualisation.

We follow data oriented principles in Rusty Tree's design, with a shallow abstraction layer that acts as an interface to operations on point data, and encoded Morton indices. In terms of lines of code, this translates into just 2565 lines of Rust and 707 lines of Python with less than 90 lines of configuration code in terms of Toml and Yaml files specifying the Rust and Python builds. This Rust and Python figure is also inclusive of all test code, and documentation. 

% Dendro, a C++ library with comparable functionality, runs to 33144 lines of code and 25962 lines of C++ header files and an additional 4550 lines of Make and CMake configuration files.

\subsubsection*{Creating Python Interfaces}

We create Python interfaces using Maturin, a tool for building and publishing Rust-based Python packages. It can do this in different ways, including the popular PyO3 \cite{pyo32022github} and rust-cpython \cite{rustcpython2022github} crates, which offer Rust bindings for Python to use Rust libraries as native extension modules, as well as interacting with Python code from Rust binaries. However Maturin also supports creating bindings using Rust's C Foreign Function Interface (CFFI), using the \pythoninline{clib} crate to expose Rust types to consumers of the C application binary interface (ABI). An example of exposing a Rust type and building a mixed Python/Rust project using the CFFI is shown in listing [TODO: exposing a Rust type to Python, perhaps just link to Rust -> Python iterator]. We choose the CFFI, as the types we wish to expose into Rust are relatively simple pointers to Rust iterators and the pointer to an MPI communicator created from Python. This also allows for a simple design with minimal external libraries, with our Python interface to be cleanly separated from Rust source code [TODO: Figure of design of Rusty Tree].

\subsubsection*{Using MPI from Rust}

MPI, via the rsMPI crate, constitutes the most complex dependency in Rusty Tree. The MPI bindings are built on top of C shim library, as well as automatically generated C/C++ compatible header files using the \pythoninline{cbindgen} crate. The shim library is used to form an equivalence between underspecified identifiers from the MPI standard, that may not be uniform across MPI implementations, and \pythoninline{cbindgen} is used to automatically generate headers for different MPI implementations. This allows rsMPI to expose an interface in Rust which covers most of MPI's core functionality for most MPI implementations, on most operating systems \footnote{This is an area of active development, and a list of currently supported features can be found here \url{https://github.com/rsmpi/rsmpi/blob/main/README.md}}. However, as it has to target multiple platforms like all other Rust crates its build is fragile, and requires extra configuration from users who might wish to deploy on unsupported or novel hardware and software targets - for example on large clusters which may have custom software environments that are difficult to change. We give an overview of how we have adapted a fork of rsMPI to compile our software on a wider range of targets in figure [TODO: Box on how to compile rsMPI on Archer and Kathleen].

This is an example of fragility in Rust builds when they rely on external libraries, however we note that this is rarely exposed to users, who can continue to consume libraries using a single line from their project's \pythoninline{Cargo.toml}. Therefore, this still marks a significant improvement from the user experience of libraries built in other compiled languages.

\subsection*{Scaling Experiments}

Figure [TODO: weak scaling of Rusty Tree on Kathleen] shows the weak scaling performance of Rusty Tree in comparison to the popular \pythoninline{p4est} library for parallel octrees, as well as the \pythoninline{dendro} library, both written in C++. In terms of API, Rusty Tree is the only available package that is fully interoperable from Python, and as we see we incur no noticeable performance hit. We therefore see that Rusty Tree provides us with malleable software, with a high-level language interface, that can be inserted into existing research projects with minimal configuration. Rusty Tree multi-platform,

\subsection*{Conclusion}

Rusty Tree is an example of Rust software that fulfills our usability criteria for scientific software. It has high-level interfaces, is designed around data and is simple to read, and importantly can be built for a variety of software and hardware environments. It's a demonstration of Rust as a viable alternative to development in Rust as opposed to C++ or Fortran, and the performance of the library using the Python interface is an example of the final interface we hope to expose to users of our fast solvers infrastructure. Users are able to access high-performance code from the comfort of Python, and if they need to add or extend functionality have a relatively simple Rust backend to read. We believe that this will encourage the widespread adoption of our library, and we hope to publish these results in an upcoming paper.
