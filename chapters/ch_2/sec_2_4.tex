Octrees are a foundational data structure for fast algorithms in $\mathbb{R}^3$. Writing software for octrees that can be distributed across parallel computing systems is challenging, demonstrated by the limited availability of open-source software \cite{BursteddeWilcoxGhattas11,fernando2020github}. This is inspite of their ubiquity in scientific computing applications from adaptive finite-element methods to many-body algorithms \cite{sundar2008bottom}. We present Rusty Tree, an implementation of MPI-distributed octrees with a complete Python interface. Rusty Tree is a proof of concept for Rust as a tool for high-performance computational science. We document our experience in developing Rusty Tree, and use it as a tool to explore the current landscape of scientific computing with Rust. From multithreading, and developing a Python interface, to distributing applications with MPI, writing Rusty Tree involved using much of Rust's scientific computing ecosystem. 

\subsection*{Parallel Octrees}

As mentioned in section \ref{sec:2_2}, there are multiple representations of octrees. We choose a linear octree representation, in which we discard interior nodes and store a list of leaves, as in our previous work PyExaFMM \cite{kailasa2022pyexafmm}. These lie in contrast to pointer based octrees in which each node stores pointers to its parent as well as children, which have a relatively higher storage cost, in addition to a synchronisation and communication overhead for parallel implementations that must keep track of pointers across nodes \cite{tu2005scalable}. 

Octrees are usually constructed `top down'. A user specifies a threshold for the maximum number of particles in a leaf node, $n_{\text{crit}}$, and a unit cube encapsulating the problem domain is refined until this is satisfied. After refinement, one can optionally `balance' adjacent leaf nodes such that adjacent nodes obey a relative size constraint. However, translating this logic to a parallel setting is challenging. Constructing octrees `top down' involve a significant data communication that come from having to synchronise nodes across processors during refinement. Balancing requires updating a chain of relationships, that can `ripple' across processors. Furthermore, as the data distribution is not known a priori, load balancing must be done as a post-processing step.

Sundar et al introduce `bottom up' tree construction \cite{sundar2008bottom}. The strategy relies on a `space filling curve' that passes through each octant in an octree, for which they choose a Morton encoding (see fig. (\ref{fig:sec_2_4:morton}) for a description of Morton encoding for an octant). Morton encodings have excellent properties for spatial decompositions. Once sorted, adjacent Morton keys encode spatial locality. Indeed traversing a sorted list of Morton keys from smallest to greatest is equivalent to a level by level traversal of an octree, starting at its root node. Similarly, a reverse order traversal is equivalent to a level by level traversal of an octree starting at its leaf level. This process is accelerated in RustyTree using the Rayon library for parallel multithreading which we examine in figure (\ref{fig:sec_2_4:rayon}). We build on Sundar et. al's original paper in two key ways. Firstly, we use HykSort \cite{sundar2013hyksort}, which reduces global data communication cost in comparison to a naive \pythoninline{MPIAllToAllv}, and use a combination of local balancing (alg. (\ref{alg:sec_2_4:balance_octree})) and parallel sorting \textit{avoid} their complex `ripple propagation' scheme to balance a distributed tree.

\begin{figure}
    \centerline{\includegraphics[width=\linewidth]{ch_2/morton.pdf}}
    \caption{How to Morton encode a node in an octree. [TODO: Extend graphic to explain Morton ordering]}
    \label{fig:sec_2_4:morton}
\end{figure}

Given a set of distributed point data spread across $n_p$ processors, our strategy is as follows:

\begin{enumerate}
    \item Generate a Morton key for each point corresponding to a user defined octree depth.
    \item Perform a parallel sort of these leaf octants using MPI, such that processors with adjacent MPI ranks have adjacent data.
    \item \textit{Linearise} leaves on each processor, such that their leaves do not overlap, in a manner that favours larger leaves over smaller ones, as in algorithm (\ref{alg:sec_2_4:linearise_octree}). 
    \item \textit{Complete} the space defined by the leaves on each processor, and find the domain occupied by their leaves, see figure [TODO: COMPLETE REGION] and algorithm (\ref{alg:sec_2_4:complete_region}). Define the coarsest nodes of this set as \textit{seeds}.
    \item Construct a \textit{block tree}, by completing the space between the seeds with the minimum spanning set of nodes, described in algorithm (\ref{alg:sec_2_4:complete_octree}). The nodes of this block tree are referred to as \textit{blocks}. This gives us a distributed coarse complete linear octree that is based on the underlying data distribution, and is described in algorithm.
    \item The load of each block is calculated by using the number of original octants calculated in step (1). The blocks are then redistributed so that each processor contains an approximately equal load, using algorithm (\ref{alg:sec_2_4:partition}).
    \item Point data is communicated to the processor containing its associated block, and the blocks are refined based on $n_{\text{crit}}$, providing the final, load balanced, linear octree tree.
    \item Optionally, the tree is \textit{balanced}. This can be done for the local subtree on each processor using algorithm (\ref{alg:sec_2_4:balance_octree}) The locally balanced trees can then be sorted in parallel again, and locally linearized as in step (3).
\end{enumerate}

This is summarised in algorithm (\ref{alg:sec_2_4:point2octree}). The complexity of this process is bounded by the parallel sorts, which for randomly distributed point data, run in $O(N_{\text{leaf}} \log (N_{\text{leaf}}))$ work and $O(\frac{N_{\text{leaf}}}{n_p} \log(\frac{N_{\text{leaf}}}{n_p}) + n_p \log (n_p))$ time where $N_{\text{leaf}}$ is the number of leaves in the final octree, and $n_p$ is the number of processors (see appendix \ref{app:a_2_complexity_tree_construction} for a derivation). This makes the efficiency of the parallel sort a bottleneck in tree construction.

\begin{figure}
    \centerline{\includegraphics[width=\linewidth]{ch_2/rayon.pdf}}
    \caption{Optimising Morton encodings using shared memory parallelism and lookup tables.}
    \label{fig:sec_2_4:rayon}
\end{figure}

\begin{algorithm}
    \caption{\textbf{Remove Overlaps From Sorted List of Octants (Sequential)}: We refer to this as \textit{Linearization}.}
    \label{alg:sec_2_4:linearise_octree}
    \begin{algorithmic}
    
        \STATE \textbf{Input}: A sorted list of octants, $W$.
        \STATE \textbf{Output}: $R$, an octree with no overlaps.
        \STATE \textbf{Work}: $O(n)$, where $n$=len($W$).
        \STATE \textbf{Storage}: $O(n)$, where $n$=len($W$).
    

        \FOR{i $\gets$ 1 to len($W$)}
            \IF {$W[i] \notin \{ \text{Ancestors}(W[i+1])\}$}
                \STATE $R \gets R + W[i]$  
            \ENDIF
        \ENDFOR
        \STATE $R\gets R+W$[len($i$)]
    \end{algorithmic}
\end{algorithm}
    

\begin{algorithm}
    \caption{\textbf{Construct a Minimal Linear Octree Between Two Octants (Sequential)}: We refer to this as \textit{Completion}.}
    \label{alg:sec_2_4:complete_region}
    \begin{algorithmic}
        \STATE \textbf{Input}: Two octants $a$ and $b$, where $a > b$ in Morton order. 
        \STATE \textbf{Output}: $R$, minimal linear octree between $a$ and $b$. 
        \STATE \textbf{Work}: $O(n \log n)$, where $n$=len($R$).
        \STATE \textbf{Storage}: $O(n)$, where $n$=len($R$).

        \FOR {$w \in W$}
            \IF {$a < w < b$ \textbf{and} $w \notin \{ \text{Ancestors}(b)\}$}
                \STATE $R \gets R + w$
            \ELSIF {$w \notin \{\text{Ancestors}(a), \text{Ancestors}(b)\}$}
                \STATE $W \gets W - w + \text{Children}(w)$
            \ENDIF
        \ENDFOR
        \STATE \text{Sort}($R$)
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \caption{\textbf{Balance a Local Octree (Sequential)}: A 2:1 balancing is enforced, such that adjacent octants are at most twice as large as each other.}
    \label{alg:sec_2_4:balance_octree}
    \begin{algorithmic}
        \STATE \textbf{Input}: A local octree $W$, on a given node.
        \STATE \textbf{Output}: $R$, a 2:1 balanced octree. 
        \STATE \textbf{Work}: $O(n \log n)$, where $n$=len($R$).
        \STATE \textbf{Storage}: $O(n)$, where $n$=len($W$).

        \STATE $R = \text{RemoveOverlaps}(W)$
        \FOR {$l \gets \text{Depth } \text{to } {1} $}
            \STATE $Q \gets \{ x \in W | \text{Level}(x) = l \}$
            \FOR {$q \in Q$}
                \FOR {$n \in \{\text{Neighbours}(q), q\}$}
                    \IF {$n \notin R$ and $\text{Parent}(n) \notin R$}
                        \STATE $R \gets R + \text{Parent}(n)$
                        \STATE $R \gets R + \text{Siblings}(\text{Parent}(n))$
                    \ENDIF
                \ENDFOR
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \caption{\textbf{Partition a Distributed List of Octants (Parallel)}.}
    \label{alg:sec_2_4:partition}
    \begin{algorithmic}
        \STATE \textbf{Input}: A distributed list of octants, $W$. 
        \STATE \textbf{Output}: The octants redistributed across processors such that the total weight on each processor is roughly the same. 
        \STATE \textbf{Work}: $O(n)$, where $n$=len($W$).
        \STATE \textbf{Storage}: $O(n)$, where $n$=len($W$).

        \STATE $S \gets$ Scan(Weight($W$))
        \IF {rank = $n_p-1$}
            TotalWeight $\gets$ max($S$)
        \ENDIF
        \STATE $\bar{w} \gets \frac{\text{TotalWeight}}{n_p}$
        \STATE $k \gets \text{TotalWeight} \mod n_p$
        $Q_{tot} \gets \emptyset$
        \FOR { $p \gets 1$ to $n_p$}
            \IF {$p \leq k$}
                \STATE $Q \gets \{ x \in W | (p-1)(\bar{w} + 1) \leq S(x) < p (\bar{w}+1) \}$
            \ELSE
                \STATE $Q \gets \{ x \in W | (p-1)\bar{w} + k < S(x) < p \bar{w} + k \}$
            \ENDIF

            \STATE $Q_{tot} \gets Q_{tot} + Q$
        \ENDFOR
        \STATE $R \gets$ Receive()
        \STATE $W \gets W-Q_{tot}+R$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\textbf{Construct a Complete Linear Octree From a Set of Seed Octants Spread Across Processors (Parallel)}}
    \label{alg:sec_2_4:complete_octree}
    \begin{algorithmic}
        \STATE \textbf{Input}: A distributed sorted list of seeds $L$.
        \STATE \textbf{Output}: $R$, a complete linear octree. 
        \STATE \textbf{Work}: $O(n \log n)$, where $n$=len($R$).
        \STATE \textbf{Storage}: $O(n)$, where $n$=len($R$).

        \STATE $R = \text{RemoveOverlaps}(L)$
        \STATE $L \gets \text{Linearise}(L)$, using algorithm (\ref{alg:sec_2_4:linearise_octree}).
        \STATE $\text{Partition}(L)$, using algorithm (\ref{alg:sec_2_4:partition}) 

        \IF {rank == 0}
            \STATE $L$.push\_front(FirstChild(FinestAncestors(DeepestFirstDescendent(root), $L$[1])))
        \ENDIF
        
        \IF {rank == $n_p-1$ }
            \STATE $L$.push\_back(LastChild(FinestAncestors(DeepestLastDescendent(root), $L$[len($L$)])))
        \ENDIF

        \IF {rank > 0}
            \STATE Send($L$[1], (rank-1))
        \ENDIF

        \IF {rank < ($n_p-1$)}
            \STATE $L$.push\_back(Receive())
        \ENDIF

        \FOR {$i \gets 1$ to (len($L$)-1)} 
            \STATE $A \gets$ CompleteRegion($L[i]$, $L[i+1]$), using algorithm (\ref{alg:sec_2_4:complete_region})
        \ENDFOR
        
        \IF {rank = $n_p-1$}
            \STATE $R \gets R+L[\text{L}]$
        \ENDIF

    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\textbf{Partitioning Octants Into Large Parallel Blocks (Parallel)}}
    \label{alg:sec_2_4:block_partition}
    \begin{algorithmic}
        \STATE \textbf{Input}: A distributed list of octants $F$.
        \STATE \textbf{Output}: A list of blocks $G$, $F$ redistributed but the relative order of the octants is preserved. 
        \STATE \textbf{Work}: $O(n)$, where $n$=len($F$).
        \STATE \textbf{Storage}: $O(n)$, where $n$=len($F$).

        \STATE $T \gets $ CompleteRegion($F$[1], $F$[len($F$)]), using algorithm (\ref{alg:sec_2_4:complete_region})
        \STATE $C \gets \{ x \in T | \forall y \in T, \text{Level}(x) \leq \text{Level}(y) \}$
        \STATE $G \gets $ CompleteOctree($C$), using algorithm (\ref{alg:sec_2_4:complete_octree})
        \FOR {$g \in G$} 
            weight($g$) $\gets $ len($F_{global} \cap \{ g,  \{ \text{Descendents (g)} \} \}$)
        \ENDFOR

        \STATE Partition($G$), using algorithm (\ref{alg:sec_2_4:partition})
        \STATE $F \gets F_{global} \cap \{ g, \{ \text{Descendants}(g) \} \forall g \in G \}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\textbf{Construct Distributed Octree (Parallel)}}
    \label{alg:sec_2_4:point2octree}
    \begin{algorithmic}
        \STATE \textbf{Input}: A distributed list of points $L$, and a parameter $n_{crit}$ specifying the maximum number of points per octant.
        \STATE \textbf{Output}: A complete linear octree, $B$. 
        \STATE \textbf{Work}: $O(n \log n)$, where $n$=len($L$).
        \STATE \textbf{Storage}: $O(n)$, where $n$=len($L$).
        
        \STATE $F \gets $ [Octant($p$, MaxDepth), $\forall p \in L$]
        \STATE ParallelSort(F)
        \STATE $B \gets $ BlockPartition($F$), using algorithm (\ref{alg:sec_2_4:block_partition})
        \FOR {$b \in B$}
            \IF {NumberOfPoints($b$) $>$ $n_{crit}$}
                \STATE $B \gets B - b + $ Children($b$)
            \ENDIF
        \ENDFOR

        \STATE \\\# Optional Balancing over subtrees, $f$.

        \IF {Balance = True}
            \FOR {$f \in F$}
                \STATE Balance($f$), using algorithm (\ref{alg:sec_2_4:balance_octree})
            \ENDFOR

            \STATE ParallelSort($F$)
            
            \FOR {$f \in F$}
                \STATE Linearise($f$), using algorithm (\ref{alg:sec_2_4:linearise_octree})
            \ENDFOR
        \ENDIF

    \end{algorithmic}
\end{algorithm}

\subsection*{Efficient Parallel Sorts}

Designing and implementing an efficient sorting algorithm that can scale to thousands of cores is difficult since it requires irregular data access, communication, and load-balance. In the original paper \cite{sundar2008bottom}, Sundar et. al use a variant of sample sort. Briefly, this approach samples elements at each processor to create a set of $n_p - 1 $ ordered `splitters', which are shared across all processors and define a set of $n_p$ buckets. This is followed by a global all-to-all communication call over all $n_p$ processors to assign elements to their corresponding bucket. Finally, a local sort is performed at each bucket to produce a globally sorted array. SampleSort is well understood. However, its performance is quite sensitive to the selection of splitters, which can result in load imbalance. Most importantly, the all-to-all key redistribution scales linearly with the number of tasks and can congest the network. As a result SampleSort may scale suboptimally, especially when the communication volume approaches the available hardware limits \cite{sundar2013hyksort}. Indeed, when tested at the largest experimental values involving $~1e9$ particles, and $n_p \geq 10^2$, the library defined \pythoninline{MPI_Alltoallv()} is not able to cope with the volume of data being sent across the network, and results in a segfault. We compare HykSort to a Rust implementation of Sample Sort in figure [TODO: Comparison of Hyksort and Samplesort (naive)], noting that HykSort provides a [TODO: performance improvement], and scales to [TODO: Max scale comparison].

An alternative approach is provided by Sundar et. al's `HykSort' \cite{sundar2013hyksort}. Here they reduce the communication overhead with the following improvement to sample sort. Instead of splitting the global array into $n_p$ buckets, they split into $k < n_p$, and recursively sort for each bucket. This situation is sketched in figure [TODO: Figure of sample sort]. We notice that at each recursion step, each task communicates with just $k$ other tasks. Indeed, for $k=2$ we recover Quicksort over a hypercube. 

Sundar et. al. also provide an algorithm to optimally select splitters, see algorithm [TODO: splitter algorithm], and a SIMD optimised local merge-sort, see algorithm [TODO: merge sort algorithm]. Upon comparison between their implementation to Rust's inbuilt sorting function in figure [TODO: Comparison of Rust with usort for local sort] we decided to use Rust's sort method in our implementation.

\subsection*{Rusty Tree}

Rusty Tree can consumed as either a Rust crate or as a Python package distributed via Conda, with uniform installation steps on different platforms. We have so far tested Rusty Tree on MacOS, RedHat [TODO: Lookup what Timo's machine uses] and [TODO: Lookup what Myriad uses]. Rusty Tree contains a full test suite, and documentation, built using Cargo, and can be found at \url{https://github.com/rusty-fast-solvers/rusty-tree}.

Example usage in Python mirrors the Rust interface [TODO: Python Vs Rust experiment]. Users are able use point data generated at runtime, or provide a HDF5 file for Rusty Tree to consume. Rusty Tree is also capable of outputting a VTK file for visualisation.

We follow data oriented principles in Rusty Tree's design, with a shallow abstraction layer that acts as an interface to operations on point data, and encoded Morton indices. In terms of lines of code, this translates into just 2565 lines of Rust and 707 lines of Python with less than 90 lines of configuration code in terms of Toml and Yaml files specifying the Rust and Python builds. This Rust and Python figure is also inclusive of all test code, and documentation. 

\subsubsection*{Creating Python Interfaces}

We create Python interfaces using Maturin, a tool for building and publishing Rust-based Python packages. It can do this in different ways, including the popular PyO3 \cite{pyo32022github} and rust-cpython \cite{rustcpython2022github} crates, which offer Rust bindings for Python to use Rust libraries as native extension modules, as well as interacting with Python code from Rust binaries. However Maturin also supports creating bindings using Rust's C Foreign Function Interface (CFFI), using the \pythoninline{clib} crate to expose Rust types to consumers of the C application binary interface (ABI). An example of exposing a Rust type and building a mixed Python/Rust project using the CFFI is shown in listing [TODO: exposing a Rust type to Python, perhaps just link to Rust -> Python iterator]. We choose the CFFI, as the types we wish to expose into Rust are relatively simple pointers to Rust iterators and the pointer to an MPI communicator created from Python. This also allows for a simple design with minimal external libraries, with our Python interface to be cleanly separated from Rust source code [TODO: Figure of design of Rusty Tree].

\subsubsection*{Using MPI from Rust}

MPI, via the rsMPI crate, constitutes the most complex dependency in Rusty Tree. The MPI bindings are built on top of C shim library, as well as automatically generated C/C++ compatible header files using the \pythoninline{cbindgen} crate. The shim library is used to form an equivalence between underspecified identifiers from the MPI standard, that may not be uniform across MPI implementations, and \pythoninline{cbindgen} is used to automatically generate headers for different MPI implementations. This allows rsMPI to expose an interface in Rust which covers most of MPI's core functionality for most MPI implementations, on most operating systems \footnote{This is an area of active development, and a list of currently supported features can be found here \url{https://github.com/rsmpi/rsmpi/blob/main/README.md}}. However, as it has to target multiple platforms like all other Rust crates its build is fragile, and requires extra configuration from users who might wish to deploy on unsupported or novel hardware and software targets - for example on large clusters which may have custom software environments that are difficult to change. We give an overview of how we have adapted a fork of rsMPI to compile our software on a wider range of targets in figure [TODO: Box on how to compile rsMPI on Archer and Kathleen].

This is an example of fragility in Rust builds when they rely on external libraries, however we note that this is rarely exposed to users, who can continue to consume libraries using a single line from their project's \pythoninline{Cargo.toml}. Therefore, this still marks a significant improvement from the user experience of libraries built in other compiled languages.

\subsection*{Scaling Experiments}

Figure [TODO: weak scaling of Rusty Tree on Kathleen] shows the weak scaling performance of Rusty Tree in comparison to the popular \pythoninline{p4est} library for parallel octrees, as well as the \pythoninline{dendro} library, both written in C++. In terms of API, Rusty Tree is the only available package that is fully interoperable from Python, and as we see we incur no noticeable performance hit. We therefore see that Rusty Tree provides us with malleable software, with a high-level language interface, that can be inserted into existing research projects with minimal configuration. Rusty Tree multi-platform,

\subsection*{Conclusion}

Rusty Tree is an example of Rust software that fulfills our usability criteria for scientific software. It has high-level interfaces, is designed around data and is simple to read, and importantly can be built for a variety of software and hardware environments. It's a demonstration of Rust as a viable alternative to development in Rust as opposed to C++ or Fortran, and the performance of the library using the Python interface is an example of the final interface we hope to expose to users of our fast solvers infrastructure. Users are able to access high-performance code from the comfort of Python, and if they need to add or extend functionality have a relatively simple Rust backend to read. We believe that this will encourage the widespread adoption of our library, and we hope to publish these results in an upcoming paper.
