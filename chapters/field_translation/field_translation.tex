\chapter{High Performance Field Translations for the kiFMM}\label{chpt:field_translation}
\thispagestyle{chaptertitle} % Force the fancy style on this page

\begin{center}
    \textit{The discussion in this chapter, including figures and diagrams, is adapted from the material first presented in \cite{kailasa2024m2ltranslationoperatorskernel} }
\end{center}

\input{chapters/field_translation/m2l.tex}


\section{Leaf Level Operators (P2P, L2P, P2M)}

- NOTE this is work taken from the green-kernels library, implemented within our group for green kernel evaluation.
- 6k loc.
- specialised for a couple of instruction sets + generic autovectorised implementation.
- How do the SIMD implementations work? Newton steps + fast inverse square root.
- sleef for special functions

From FMM side
- how are we blocking targets over threads?
- what are the implications of ultrafast direct calculations on CPU, and potential for even faster on GPU (low-precision).
- i.e. majority of computation can be handled directly quite effectively. Minimise M2L, and makes direct matrix compression techniques more attractive. Could handle these rapidly on the CPU, and especially in H100-like systems with UMA, offload expensive direct computation to GPU.
- how could we translate our current codes to a GPU for P2P, what are the trade-offs? Would memory transfer mean that it's never worth it?

\section{Parent to Child Operators (M2M, L2L)}

- Formulation as BLAS3
    - specifically I want to show the exact way that this is done
    - i.e. using morton-like (at the level of a level) encoding to lookup siblings/sets of siblings at once and apply BLAS3.
    - block sizes determined heuristically for a given architecture, but could in principle be estimate from available L2/L3 cache sizes.
    - storage, i.e. relative between parent and child, especially note that extra required if different expansion order taken per level.

\section{Field Translation in a Distributed Setting}

- Communication intensive MPI FMM phases are related to ghost data communication for the M2L and P2P (All other operators are inherently local).

- Ibeid et. al. provide estimates of communication complexity based on halos of both of these operators

- From Ibeid, add a derivation.

- How are these achieved in practice? Global/Local split, introduced by Abduljabbar and Yokota.

- Suggestive of hierarchical communication pattern.

- However, this is perhaps over-complicated.

- Domains of each processor are known via all-reduce. Therefore a-priori know exactly where all elements of locally contained interaction lists lie at problem setup.

- Given massive core-count, and relatively small total node count on pre-exascale systems like Archer 2 which are likely to persist. Global/local split can be further simplified - just choose a number of nominated nodes (even just one) for the global tree computation. Instead of hierarchical exchange, calculate required data exchange as a precomputation step, and can then tune chunk-wise data exchange over all data using neighbourhood communicators. Much simpler to setup than multiple function calls at each level of the hierarchy, and allows for finer tuning of data exchange.

- Note, each AMD Epyc node on Archer 2 can easily handle problem sizes O(1e7) points in ~0.5s in double precision.

