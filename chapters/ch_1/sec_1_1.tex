The motivation behind the development of the original fast multipole method (FMM), was the calculation of potentials in $N$-body problems,

\begin{flalign}
    \label{eq:n_body:sec:1_1}
    \phi_j = \sum_{i=1}^N K(x_i, x_j)q_i
\end{flalign}

Consider electrostatics, or gravitation, where $q_i$ is a point charge or mass, and the kernels are of the form $K(x,y) = log |x-y|$ in $\mathbb{R}^2$, or $K(x,y) = \frac{1}{|x-y|}$ in $\mathbb{R}^3$. Similar sums appear in the discretised form of boundary integral equation (BIE) formulations for elliptic partial differential equations (PDEs), which are the example that motivates our research. Generically, an integral equation formulation can be written as,

\begin{flalign}
    \label{eq:generic_int_equation:sec:1_1}
    a(x)u(x) + b(x) \int_\Omega K(x, y)c(y)u(y)dy = f(x), \> \> x \in \Omega \subset \mathbb{R}^d
\end{flalign}

where the dimension $d = 2$ or $3$. The functions $a(x)$, $b(x)$ and $c(y)$ are given and linked to the parameters of a problem, $K(x,y)$ is some known kernel function and $f(x)$ is a known right hand side, $K(x,y)$ is associated with the PDE - either its Green's function, or the derivative. This is a very general formulation, and includes common problems such as the Laplace and Helmholtz equations. Upon discretisation with an appropriate method, for example the Nystr√∂m or Galerkin methods, we obtain a linear system of the form,

\begin{flalign}
    \label{eq:linear_system:sec:1_1}
    \mathsf{K} u = f
\end{flalign}

The key feature of this linear system is that $\mathsf{K}$ is \textit{dense}, with non-zero off-diagonal elements. Such problems are also \textit{globally data dependent}, in the sense that the calculation at each matrix element of the discretized system in the depends on all other elements. This density made numerical methods based on boundary integral equations prohibitively expensive prior to the discovery of so called `fast algorithms', of which the FMM is the prototypical example. The naive computational complexity of storing a dense matrix, or calculating its matrix vector product is $O(N^2)$, and the complexity of finding its inverse is $O(N^3)$ with linear algebra techniques such as LU decomposition, where $N$ is the number unknowns. 

The critical insight behind the FMM, and other fast algorithms, is that one can compress physically distant interactions by utilising the rapid decay behaviour of the problem's kernel. A compressed `low rank' representation can be sought in this situation, displayed in figure (\ref{fig:low_rank:sec_1_1}) for $\mathbb{R}^2$.

Using the FMM the best case the matrix vector product described by (\ref{eq:n_body:sec:1_1}) and (\ref{eq:linear_system:sec:1_1}) can be computed in just $O(N)$ flops and stored with $O(N)$ memory. Fast algorithm based matrix inversion techniques display similarly optimal scaling in the best case. Given the wide applicability of boundary integral equations to natural sciences, from acoustics \cite{wolf2011aeroacoustic,hao2015efficient} and electrostatics \cite{wang2021high} to electromagnetics \cite{darve2004fast} fluid dynamics \cite{rahimian2010petascale} and earth science \cite{chaillat2008multi}. Fast algorithms can be seen to have dramatically brought within reach large scale simulations of a wide class of scientific and engineering problems. The applications of FMMs aren't restricted to BIEs, as (\ref{eq:n_body:sec:1_1}) shares its form with the kernel summations often found in statistical applications, the FMM has found uses in and computational statistics \cite{ambikasaran2013large}, machine learning \cite{lee2012distributed} and Kalman filtering \cite{li2014kalman}. The uniting feature of these applications is their global data dependency. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{ch_1/low_rank.pdf}
    \caption{Given two boxes in $\mathbb{R}^2$, $\mathcal{B}_1$, $\mathcal{B}_2$, which enclose corresponding degrees of freedom, off diagonal blocks in the linear system matrix $\mathsf{K}_{\mathcal{B}_1\mathcal{B}_2}$ and $\mathsf{K}_{\mathcal{B}_2\mathcal{B}_1}$ are considered low-rank for the FMM when separated by a distance at least equal to their diameter, this is also known as `strong admissibility'. Figure adapted from \cite{minden2017recursive}.}
    \label{fig:low_rank:sec_1_1}
\end{figure}

Recent decades have seen the development of numerous mathematical techniques for the computation of fast algorithms. However given their broad applicability across various fields of science and engineering, this has not been met with a commensurate development of black box open-source software solutions which are easy to use and deploy by non-experts. This is not to say that there is an absence of research software for the FMM \cite{kailasa2022pyexafmm, exafmm,wang2021exafmm,malhotra2015pvfmm, fmm3d}, or fast matrix inversion \cite{fmm3dbie, strongskel, ho2020flam}. However, the software landscape is heavily fragmented, codes often arising out of a software or mathematical investigation with infrequent maintenance or development post-publication. Few attempts have been made to re-use data structures, or application programming interfaces (APIs) between projects, and source code is often poorly documented leading to little to no interoperability between projects. Furthermore, as codes are often written in compiled languages such as Fortran \cite{fmm3d} or C++ \cite{malhotra2015pvfmm, exafmm,wang2021exafmm}, there is a relatively high software engineering barrier entry for community contributions, further discouraging widespread adoption amongst non-specialist academics and industry practitioners. Additionally, significant domain specific expertise in numerical analysis is required by users to discern the subtle differences between fast algorithm implementations, or indeed to write one independently. 

Computer hardware and architectures continue to advance concurrently with advances in numerical algorithms. Recently, the exascale benchmark (capable of $10^{18}$ flops) was achieved by Oak Ridge National Labs' Frontier machine\footnote{https://www.olcf.ornl.gov/frontier/}. With  9,472 AMD 64 core Trento nodes with a total of 606,208 compute cores, alongside 37,888 Radeon Instinct GPUs with a total of 8,335,360 cores, programming fast algorithms with their inbuilt global data dependency is challenging at a software level due to the communication bottlenecks imposed by the necessary all to all communications. Furthermore, the dense matrix operations required by fast algorithms require delicate tuning to fully take advantage of memory hierarchies on each node. Currently there exist very few open-source fast algorithm implementations that are capable of being deployed on parallel machines \cite{malhotra2015pvfmm, exafmm}, or take advantage of a heterogenous CPU/GPU environments \cite{exafmm}. In fact for fast inverses there doesn't yet exist an open-source parallel implementation. Furthermore, developers must using existing codes must employ careful consideration in order to successfully compile the software in each new hardware environment they encounter, from desktop workstations to supercomputing clusters.

Resultantly, researchers who may want to write application code that takes advantage of fast algorithms as a black box without the necessary software or numerical analysis expertise to implement their own have few choices, and fewer still in a distributed computing setting. Identifying this as a significant barrier to entry for the adoption of fast algorithms in the wider community, we propose a new unified framework for fast algorithms, beginning with an implementation of a parallel FMM, which we introduce in the following section, designed for modern large scale supercomputing clusters. We emphasise our focus on ergonomic and malleable code, such that our code is easy to edit and deploy on a multitude of architectures while still achieving good scaling. With a key target application being the simulation of exascale boundary integral problems for electromagnetics, specified by Maxwell's equations.
