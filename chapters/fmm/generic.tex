
\section{Fast Multipole Methods}\label{chpt:fmm:sec:generic}

We use the case of evaluating electrostatic potentials to motivate the \acrshort{fmm} for non-oscillatory problems, mirroring the original presentation \cite{greengard1987fast}. Consider the electric field, $\mathbf{E}$ due to a static charge distribution $q(\Ybf)$ which is supported over some finite domain $\Ybf \in \Omega \subset \Rd$. It can be defined in terms of a scalar potential $\phi$.

\begin{equation*}
\mathbf{E} = -\nabla \phi
\end{equation*}

which itself can be seen to satisfy Poisson's equation,

\begin{equation*}
    \begin{cases}
        - \Delta \phi(\Xbf) = q(\Xbf), \> \> \text{  for } x\in \Rd \\
        \underset{|x| \rightarrow \infty}{\lim } u(\Xbf) = 0
    \end{cases}
\end{equation*}


where $d=2,3$ in problems of interest.

We can write the evaluation of the potential at a point $\Xbf$ as a convolution of the source with the fundamental solution of the Poisson equation,

such that,

\begin{equation}
\phi(\Xbf) = \int_{\Rd} K(\Xbf-\Ybf)q(\Ybf) d\Ybf, \> \> \Xbf \in \Rd
\end{equation}\label{eq:chpt:fmm:laplace_potential_integral}

Under an appropriate discretisation, where care is taken to appropriately handle the singularity in the Laplace kernel (\ref{eq:chpt:introduction:laplace_kernel}), we see that this integral corresponds to a matrix vector multiplication, where the matrix is \textit{dense}, i.e. it consists only of non-zero entries.

As we are principally concerned with the simpler problem of evaluating the potential due to a discrete charge distribution, with $N$ charges we can replace $q(\Ybf)$ with $\{ q(\Ybf_j) \}_{j=1}^N$ associated with \textit{source particles} $\{\Ybf_j\}_{j=1}^N \in \Rd$, the integral for potential evaluated at $M$ \textit{target particles}, $\{\Xbf_i \}_{i=1}^M \in \Rd$ becomes a discrete sum,

\begin{equation}
    \phi(\Xbf_i) = \sum_{j=1}^N K(\Xbf_i-\Ybf_j)q(\Ybf_j), \> \> i = 1,...,M
    \label{eq:chpt:fmm:laplace_potential_sum}
\end{equation}

where we can handle the singularity by setting,

\begin{equation}
    K(\Xbf_i - \Ybf_j) = \begin{cases}
        0, \> \> \Xbf_i = \Ybf_j \\
        K(\Xbf_i - \Ybf_j), \text{  otherwise}
    \end{cases}
\end{equation}


We see that the sum (\ref{eq:chpt:fmm:laplace_potential_sum}) corresponds to a dense matrix vector multiplication,

\begin{equation}
    \phi = K q
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{introduction/degenerate_kernel.pdf}
    \caption{A set of source and target particle cluster, where the width of each cluster is significantly less than the distance separating them, $d_s, d_t \ll D$.}
    \label{fig:chpt:fmm:degenerate_kernel}
\end{figure}

Naively computed this requires $\bigO{MN}$ operations, where in general the source and target particles may correspond to the same set. The \acrshort{fmm} relies on a \textit{degenerate} approximation of the interaction kernel when subsets, or \textit{clusters}, of source and target particles are sufficiently separated as sketched in Figure \ref{fig:chpt:fmm:degenerate_kernel}. Following the discussion in \cite{kailasa2024m2ltranslationoperatorskernel} the sum (\ref{eq:chpt:fmm:laplace_potential_sum}) can be written as,

\begin{equation}
    \phi(\Xbf_i) \approx \sum_{p=1}^P \sum_{j=1}^N A_p(\Xbf_i) B_p(\Ybf_j)q(\Ybf_j), \> \> i = 1,...,M
    \label{eq:chpt:fmm:degenerate_kernel}
\end{equation}

where we call $P$ the expansion order, taken such that $P \ll N$, $P \ll M$. The functions $A_p$ and $B_p$ are defined by the approximation scheme used by a particular approach for the \acrshort{fmm}, in the original presentation the calculation,

\begin{equation}
    \hat{q}_p = \sum_{j=1}^N B_p(\Ybf_j)q(\Ybf_j)
\end{equation}

Corresponded to the coefficients of an order $P$ multipole expansion due to the source charges. Following which the potential is approximated by,

\begin{equation}
    \phi(\Xbf_i) \approx \sum_{p=1}^P A_p(\Xbf_i)\hat{q}_p, \> \> i = 1,...,M
\end{equation}

at the target particles. The approximation of the potential with this scheme can be seen to require $\bigO{P(M+N)}$ operations. The accuracy of this approximation scheme, and the error bounds provided by the \acrshort{fmm}, depends on the distance between the source and target clusters remaining large relative to their width. This condition is often referred to as an \textit{admissibility condition} in the literature. \acrshort{fmm}s therefore split the sum (\ref{eq:chpt:fmm:laplace_potential_sum}) into \textit{near} and \textit{far} components when considering arbitrary clusters of source and target particles,

\begin{equation}
    \phi(\Xbf_i) = \sum_{\Ybf_j \in \text{Near}(\Xbf_i)} K(\Xbf_i, \Ybf_j) q(\Ybf_j) +  \sum_{\Ybf_j \in \text{Far}(\Xbf_i)} K(\Xbf_i, \Ybf_j) q(\Ybf_j), \> \> i=1,..,M
    \label{eq:chpt:fmm:near_far_split}
\end{equation}

In cases where a source and target cluster can be considered \textit{admissable}, i.e. the source cluster is considered in the \textit{far field} of the target cluster such that each $\Ybf_j \in \text{Far}(\Xbf_j)$, we apply the approximation (\ref{eq:chpt:fmm:degenerate_kernel}). However, when a source and target cluster are \textit{inadmissable}, such that the source cluster is considered in the \textit{near field} of a target cluster such that each $\Ybf_j \in \text{Near}(\Xbf_j)$ we are left to evaluate the sum directly via (\ref{eq:chpt:fmm:laplace_potential_sum}).

The notion of admissability is made more concrete by reference to a data structure chosen to discretise the problem. For the \acrshort{fmm} quadtrees and octrees are commonly used in two and three dimensions respectively. These are data structures in which a $d$-dimensional bounding box is used to cover the source and target particles, and is recursively divided into $2^d$ `child' boxes. This process can be either `adaptive' or `uniform'. In the former case, the box is divided until a user defined threshold defining the maximum number of points per terminal leaf box is reached, which can lead to adjacent boxes of differing sizes and is able to closely model extreme particle distributions. In the latter case, boxes are divided such that each leaf box is of the same size, specified by a user defined parameter controlling the maximum depth of the tree.


- In order to achieve its $\bigO{N}$ complexity the \acrshort{fmm} is structured to reduce to a minimum the number of sums evaluated between inadmissable clusters.

- Algorithm sketch, note on adaptivity and how it's defined and achieved (weak and strong)

- sketch of complexity for full generic algorithm

- The power of the FMM's original insight was the split of near and far interactions, and the usage of `interaction lists' to describe the interactions relative to each box. Furthermore the organisation into a hierarchical algorithm provide the tools for achieving linear complexity for non-oscillatory problem key to which is the ability to translate between multipole and local expansion representations. These three features (i) the construction of interaction lists and (ii) the low-rank approximation scheme used to express the field due to a collection of charges and (iii) the methods used to translate between these operators principallly define all existing FMM variants, which retain a common recursive algorithmic structure.

- From a computational perspective, the FMM offers even more features that combine with those above that result in algorithmic variants, the main ones being approaches to (i) tree construction, and resulting data layout and access schemes (ii) the approach to parallelisation and distribution across nodes (iii) the approach to mapping algorithmic subcomponents to take advantage of high-performance software and hardware.  This has resulted in a diverse set of algorithmic approaches, some of which we review in Section \ref{chpt:fmm:sec:algorithm_zoo}, all with their own benefits and trade-offs between implementational and algorithmic complexities.

- There are large lurking constants in FMM.

- Aluru, some arguments that the original FMM was not O(N) for Laplace
- Kurzak paper, in the 00s some people even thought that FMM only competetive with Ewald/treecodes at 10e62 particles, completely unfeasible
- disproved by a series of landmark codes and developments, which showed that *particular* FMMs could harness petascale compute resources.

- Give a break down table for each operator in analytical, and kernel independent cases (bbFMM and kiFMM) without acceleration, in 3D, and memory accesses total

- Have a breakdown table of each approach *with acceleration* in 3D, and memory accesses (total) required

- Despite most schemes in principle having a high flop/byte ratio, in practice the M2L by construction is memory limited, due to the non-contiguous nature of interaction lists.

- In addition to practical significance of each approach due to requirements of data access on modern CPUs, Kurzak / Dongarra paper reference on how rules have changed.

- This was addressed in a series of papers in the 90s/2000s to address this with new forms of translation operators, that reduced the asymptotic complexity of the far-field evaluations, some of which we will cover in chapter on field translations.

- Principally, can be grouped into two broad camps: analytical and algebraic. The latter of which coincide with the H matrix approach.

- Distinguishing features math: compression scheme for low rank sub-blocks
    - alg vs analytical schemes. admissability
        - algebraic
            - ACA, SVD, QR, Randomised, Krylov.
            - interpolation basis
                - MFS, Cheb
        - analytical
            - expansions
                - Sph Harm, Cartesian (Taylor)
            - compression
                - diagonalisation, plane wave/exponential
    - hierarchical block structure (HSS/HODLR/H/H2).
        - FMM is an instance of H2. analytical FMM is H2 but unrolled loop.

- Distinguishing features CS
    - Data access
    - tree (ORB vs Octree)
        - Octree:
            - pointer based
            - bottom up
        - ORB scheme, bounding boxes (square etc)
            - Abduljabbar
    - hardware
        - GPU support, het, CPU only
        - SIMD
    - interaction list data access
        - linearisation of the tree.
        - ghost exchange in distributed memory.
    - software
        - asych
        - runtime model
        - threading model
    - software platform
        - language and platform support, performance.
        - ability to support all the variants listed above, and be flexible to swap them out and compare them as toolbox for understanding the FMM.

- Combinatorial explosion of FMM variants, very little work on unifying the work that has been done. Unclear of which the best approaches even are. People find an approach and stick to it. Especially in the context of rapidly changing hardware environment, where many previous acceleration schemes based on optimal flops may no longer be optimal.

- Heuristically, the best approach in practice is likely to be one that maximises flop/bytes, while simulatneously linearising interaction list data access to take advantage of modern cache hierarchies, must also be tunable in terms of building up cache.

- The actual expansion scheme is also of relevance, as will reflect the nature of the flops
    - e.g. are we computing spherical harmonics and other special functions
    - however, the consequence of this is likely to matter less than admissability choices, and linearisation of data access schemes.

- Have ambikasaran diagram here for matrix decomposition variants as it's very clear.




