
\section{Oscillatory Fast Multipole Methods}\label{chpt:fmm:sec:oscillatory}

The crucial feature of the Laplace kernel (\ref{eq:chpt:introduction:laplace_kernel}) is the fact that far-field interactions (\ref{eq:chpt:fmm:near_far_split}) can be considered `low rank', and therefore amenable to compression. Importantly for (\ref{eq:chpt:introduction:laplace_kernel}) the rank of a given interaction between two boxes is scale invariant, and only depends on their relative positions.

However, for problems described by the Helmholtz kernel,

\begin{equation}
    K(\mathbf{x-y}) = \begin{cases}
        \frac{i}{4} H_0^{(1)}(k |\mathbf{x-y}|)  \text{, $d$ = 2}\\
         \frac{e^{ik |\mathbf{x-y}|}}{4\pi |\mathbf{x-y}|}  \text{, $d$ = 3}
    \end{cases}
    \label{eq:chpt:fmm:helmholtz_kernel}
\end{equation}

where $d$ is the spatial dimension, $\Xbf, \Ybf \in \Rd$, $k$ is the wavenumber, $H_0^{(1)}$ is the Hankel function of the first kind of order 0. The rank of interactions is no longer scale invariant, and indeed grows with box size. To see why this may be, consider the case for $d=3$. From theorem 2.11 in \cite{colton1998inverse}, we can express the Helmholtz kernel as a separable series,

\begin{equation}
    \frac{e^{ik|\Xbf - \Ybf|}}{4\pi |\Xbf - \Ybf|} = ik \sum_{p=0}^\infty \sum_{-p}^p h_p^{(1)}(k|\Xbf|) Y_p^m(\frac{\Xbf}{|\Xbf|})j_p(k|\Ybf|) \overline{Y_p^m(\frac{\Ybf}{|\Ybf|})}
\end{equation}

Where $k$ is the wavenumber, $Y_p^m$, for $m=-n,...,p$ $p=0,1,...$ are set of orthonormal spherical harmonics, and $|\Xbf| > |\Ybf|$, $j_p$ is the spherical Bessel function of order $p$ and $h_p^{(1)}$ is the spherical Hankel function of the first kind of order $p$.

In which case, an expression of the form (\ref{eq:chpt:fmm:degenerate_kernel}) for the Helmholtz potential evaluated at a set of $M$ target particles due to a set of $N$ source particles

\begin{equation}
    \phi(\Xbf_i) \approx \sum_{p=1}^P \sum_{m=-p}^p \sum_{j=1}^N A_p(x_i) B_p(y_j) q(y_j), i=1,...,M
\end{equation}

where we've truncated the expansion to $P$ terms, known as the expansion order, and $A$ and $B$ are functions of the target and source particle positions only, respectively. We see that in this case for increasing expansion order, the number of terms in the sum grows quadratically. Though a demonstration is out of scope for this thesis, we mention that the number of terms $P$ required to observe convergence in the above sum is proportional to $kD$

\begin{equation}
    P \approx kD
    \label{eq:chpt:fmm:p_kd}
\end{equation}

Some work for schemes that rely on the \acrshort{mfs} is presented in \cite{barnett2008stability}. Therefore, in the \acrshort{fmm} for oscillatory problems interactions between boxes can be seen to have ranks growing quadratically, proportionally to $(kD)^2$, for increasing box size in 3D.

For schemes based on \acrshort{mfs} as the \acrshort{kifmm} used in our software, the quadratic relationship between rank growth and box size is observed by noting that the condition for convergence (\ref{eq:chpt:fmm:p_kd}) corresponds to a fixed number of points per square wavelength (in 3D), therefore the rank, which is proportional to the number of points used to discretise the equivalent surfaces, can be seen to grow quadratically with increasing box size.

- If one were to use an ordinary scheme for the FMM, this would correspond to a doubling of the expansion order with each level

In order to handle this growth in rank while retaining a fast algorithm, analytical approaches were introduced that rely on exponential expansions and diagonal forms

... Some more specifics here about what these enable, i.e. they don't change the rank, but in this representation which is relatively cheap to get to, the multipole to local translation operators are diagonal and therefore cheap to compute.

- Much of the algorithmic structure remains the same.

- The `wide-band' FMM introduced by Cheng and co-workers, split of when and where to use each representation.

- complexity estimate for analytical high frequency FMMs

... There has also been some work [engquist and ying, second darve and messner paper] on extending kernel Independent methods to handle oscillatory problems, which have been demonstrated to achieve the same complexity.

- How do these work? Based on a cone-admissability condition.

- However, as we've seen modern architectures offer favourable environment for direct computations, and therefore we are able to get away with relatively shallow octree data structures. As the octrees are shallow, the compounding effect of rank growth with decreasing level/increasing box size, is bounded. If one could vary the expansion order by level, in order to remain in the convergence regime, one could continue to use the ordinary kiFMM, however you would lose the $N \log{N}$ algorithmic scaling.

- We have an implementation that allows us to vary $P$ by level, therefore can keep observed accuracy constant while increasing $P$ as boxes get larger.

- Some kind of experiment showing the convergence graph with increasing P, in the geometric then convergence regimes.

- Scaling graph vs $\bigO{N}$ $\bigO{N \log{N}}$ - need to check. However, the actual complexity will increase quartically with $k$ as growing number of terms with level,

e.g. \acrshort{m2m}

Diameter at level $l$ is $$ D_l = D_d 2^{d-l}$$

where $d$ is the depth of the octree.

$$
\text{Cost at level $l$} = O(N_l^2) = O((kD_l)^4)
$$

Where the rank of the M2M matrices is $N_l$ at level $l$ and $D_l$ is the box diameter at level $l$ and $k$ is the wave number

Written in terms of the diameter of the finest boxes at depth $d$,

$$
\text{Cost at level $l$ } = O((k D_d 2^{d-l})^4) = O(k^4 D_d^4 2^{4(d-l)})
$$


\begin{flalign}
    \text{Total Cost } &= O \left( k^4 D_d^4 2^{4d} \sum_{l=0}^d  2^{-4l} \right) \\
    &=O \left( k^4 D_d^4 2^{4d} \frac{16}{15}(1- \frac{1}{16^{d+1}}) \right)
\end{flalign}

Scales quartically with $k$ and exponentially with depth $d$ - which is used to balance P2P cost.

Consider that for $\sim N$ leaf boxes, the depth of the tree is given by $d = \log_8{N}$,

$$
d = \log_8{N} = \frac{1}{3} \log_2{N}
$$

Substituting into the complexity estimate and ignoring small terms,

$$
\text{Total Cost } = O \left( \frac{16}{15}  k^4 D_d^4 N^{4/3}  \right)
$$

- For a fixed $k$, and $D_d$, we have an $O(N^{4/3})$ algorithm for computing all \acrshort{m2m}, as other operators \acrshort{m2l} and \acrshort{l2l} will result in very similar complexity estimates as the \acrshort{m2l} will only differ by a constant. So cost of \acrshort{fmm} with this scheme of increasing expansion order by level is of $O(N^{4/3})$, but constants are determined by $kD$ which scales quartically, so only manageable for relatively shallow trees and moderate wavenumbers.

- In terms of complexity not as good as specialised algorithm, however as the runtime performance will largely be driven by quality of data access schemes, and kernel evaluations, we consider to what extent the machinery of the \acrshort{kifmm} can be re-used for moderate frequency problems. How high can wavenumber be increased, before the lack of linear scaling leads to poor overall runtimes? We are able to test this as in our implementation we are able to tune expansion order by level. This is something we explore in Section \ref{chpt:experiments:sec:helmholtz}.

