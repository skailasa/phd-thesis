
\chapter{Introduction}\label{chpt:introduction}
\thispagestyle{chaptertitle} % Force the fancy style on this page
Since its introduction in the late 1980s by Greengard and Rokhlin \cite{greengard1987fast}, the \acrfull{fmm} has become a hallmark algorithm of scientific computing often cited as one of the `top 10' algorithmic advances of the past century \cite{cipra2000best}. The problem it addresses was originally motivated by $N$ particle simulations in which the interactions are \textit{global} but with a strongly decaying property. Motivating examples being $N$ particles interacting via gravity or electrostatic forces. In these cases, as well as for interactions delineated by particular interaction \textit{kernels}, interactions between distant \textit{clusters} of particles can be represented by truncated series expansions. This is indeed where the name for the original presentation originated, as multipole expansions derived from the fundamental solution of the Poisson equation, often called the \textit{Laplace kernel} in \acrshort{fmm} literature were used to form these truncated series expansions,


\begin{equation}
    K(\mathbf{x-y}) =  \begin{cases}
        &= -\frac{1}{2\pi} \log(|\mathbf{x-y}|) \text{, $d$ = 2} \\
        &= \frac{1}{4\pi | \mathbf{x-y}|} \text{, $d$=3 }
    \end{cases}
    \label{eq:chpt:introduction:laplace_kernel}
\end{equation}


where $\Xbf, \Ybf \in \Rd$ and $d$ is the spatial dimension. Furthermore, by using a hierarchical discretisation for the problem domain, increasingly distant interactions can be captured while still using truncated sums to express the potential due to particles contained within each subdomain in the hierarchy. With this, the \acrshort{fmm} is able to reduce the $\bigO{N^2}$ operations required to evaluate the potentials at each of $N$ particles due to all other particles into an algorithm requiring just $\bigO{N}$ for problems described by the Laplace kernel (\ref{eq:chpt:introduction:sec:motivation:laplace_kernel}). The crucial advantage of the \acrshort{fmm} is that it comes equipped with rigorous error estimates, which guarantee exponential convergence with increasing numbers of series terms used in the truncated expansion, such that the problem could be evaluated to any desired accuracy while retaining the $\bigO{N}$ complexity bound for number of operations.

In the preceding decades, the \acrshort{fmm} has been extended with numerous variants that utilise a similar algorithmic framework, often with associated software efforts. Despite this, unlocking the highest available performance for practical \acrshort{fmm} implementations remains an active area of research. Principally this can be attributed to the dramatic changes in the landscape of computing technologies in the decades since the algorithm's first introduction.

Since the end of Dennard Scaling\footnote{First articulated in 1974 by Robert Dennard, Dennard scaling described how as transistors were miniaturised their power density remarkably was able to be maintained as a constant. This held true until the mid 2000s, at which point physical limits on heat dissipation and leakage current lead to power efficiency gains via miniaturisation plateauing despite the steady increase in miniaturisation described by Moore's law, marking the end of Dennard scaling. This in turn lead to the growth of multicore processors and specialised hardware accelerators, as a way to increase available computing performance without increasing power consumption.} in the 2000s, hardware design and development has focussed on enhancing parallelism. Hierarchical levels of parallelism are now available to programmers representing a significant departure from the \acrfull{sisd} systems which existed at the time of the \acrshort{fmm}'s introduction. From true hardware parallelism available via \acrfull{clp}, where multiple \acrfull{cpu} cores are able to independently execute tasks or threads simultaneously in a \acrfull{mimd} fashion, to \acrfull{tlp}, extended by technologies such as \textit{hyperthreading}, whereby each physical core is able to run multiple threads sharing an address space simultaneously, which are optimally scheduled by the operating system. \acrfull{dlp}, whereby the same operation is applied to multiple data elements simultaneously is made available to programmers via the \acrfull{simd} execution model represented by \textit{vector instruction sets} available to take advantage of special hardware registers on modern \glspl{cpu}, as well as via the the \acrfull{simt} execution model of modern \glspl{gpu}.

The importance of simultaneously exploiting multiple levels of parallelism for achieving performance with modern software, and the increasing disparity between memory bandwidth and compute resources \cite{dongarra2017extreme}, makes carefully organising memory access through the deep hierarchies of modern memory technologies increasingly the key bottleneck to unlocking performance in both a shared memory and distributed setting to ensure that memory movements do not dominate runtimes. In this context the expense of pairwise evaluations of (\ref{eq:chpt:introduction:laplace_kernel}) addressed by the \acrshort{fmm} are increasingly of less relative importance for even moderately sized problems involving hundreds of thousands of source and target particles in shared memory\footnote{We demonstrate this with specific benchmarks in Chapter \ref{chpt:experiments} for a selection of \acrshort{cpu} architectures.}, as the direct evaluation is embarrassingly parallel over each target particle with significant opportunity for memory re-use and well suited to the \acrshort{simd} or \acrshort{simt} execution models of modern \glspl{cpu} or \glspl{gpu}, respectively. Despite the $\bigO{N}$ complexity bound offered by the \acrshort{fmm}, the operators from which it is composed contain practically significant constants and implicit non-contiguous memory accesses, making achieving practical performance challenging particularly in three dimensions.

Software efforts for \glspl{fmm} were a particular focus of research activity in the 2010s, with particularly prominent examples being the ExaFMM project \cite{barba2011exafmm, wang2021exafmm},and PVFMM \cite{malhotra2015pvfmm}. Indeed, software for the \acrshort{fmm} and closely related methods have cumulatively been the recipients of three ACM Gordon Bell awards \cite{bell2017look}. Particular recent focii have been to examine how heterogenous architectures can be taken advantage of \cite{malhotra2015pvfmm}, whereby \glspl{cpu} and \glspl{gpu} are used in concert for data organisation and \acrshort{cpu} bound components of the algorithm respectively, as well as taking advantage of existing parallel runtimes for achieving task-based parallelism to high effect especially in a distributed memory setting \cite{bramas2020tbfmm, agullo2014task}.

The collective weakness of existing software efforts however is their brittleness. The complexity for developing high-performance software in a research setting results in softwares that are strongly adapted for a particular algorithmic approach, hardware architecture, or runtime system, often with the goal of achieving a particular benchmark or demonstrating the utility of a particular technique. Relatively little software is under active maintenance, with clear technical documentation on how performance was achieved, and fewer still open their subcomponents for extension, have downstream users and simple user interfaces. As a result, it is difficult to compare algorithmic and software optimisations taken by different implementations, and contrast their relative merits, as well as to see how particular approaches adapt to advances in available hardware and software.

Achieving the greatest available performance, apart from inherently being of scientific interest, enables larger and more detailed scientific simulations. The \acrshort{fmm} has been deployed as a core numerical method in the solution of elliptic \acrfullpl{pde} when formulated as boundary or volume integral equations in combination with iterative methods, therefore acts as a crucial subcomponent in derived solvers applied to a vast array of problems in computational physics, from acoustics \cite{hao2015efficient}, and electromagnetics \cite{darve2004fast} to fluid dynamics \cite{darve2004fast}, and biomolecular electrostatics \cite{yokota2011biomolecular, wang2021high}. Generalised variants of the \acrshort{fmm} have also been applied in other areas requiring fast kernel summation arising in computational statistics, such as in the widely applied Kalman filter \cite{li2014kalman}, modelling Gaussian processes \cite{ambikasaran2014fast}, and machine learning \cite{gray2000n, march2017far}. Indeed, faster $N$ particle kernel summations, of which the \acrshort{fmm} is an example, have been identified as a key benchmark operation for optimisation for in \acrshort{hpc} due to their broad utility \cite{asanovic2006landscape}, demonstrating the importance of developing performant open-source software for \acrshortpl{fmm}.

However scientific software that hopes to achieve widespread adoption must also focus on \textit{usability} in addition to performance. Which in this context means a software that is easy to deploy on current and emerging hardware and software environments, can be interfaced by common programming languages, and is open to extension to new algorithmic approaches and implementations, while remaining highly-performant. This in turn makes the structure of the software itself an object of study.

The focus of this thesis can therefore be summarised as the development of a \textit{platform} for developing \acrshortpl{fmm}, in particular the \acrfull{kifmm}, a `black box' variant of the \acrshort{fmm} which doesn't rely on explicit series expansions of the fundamental solution and only on its evaluation, that thrives even after the conclusion of this research project. Our software consists of re-usable subcomponents, and acts as a useful tool for algorithmic investigation while being capable of state of the art performance. We demonstrate this through studies into both the design as well as the application of our software in investigating the implementation of algorithmic subcomponents of the \acrshort{kifmm}.

We begin in Chapter \ref{chpt:fmm} by reviewing the literature on algorithmic and software developments for \acrshortpl{fmm}, and related methods such as $\H$ matrices, with a particular focus on the \acrshort{kifmm} currently implemented by our software. We review the computational structure general of these algorithms, and identify the requirements and bottlenecks for fast implementations for non-oscillatory and oscillatory problems, as well as in a distributed memory setting. We also review recent software developments, and describe the current open-source landscape.

In Chapter \ref{chpt:field_translation} we present an application of our software through an investigation of optimisations for the field translations for the \acrshort{kifmm}, devoting the most focus to the \acrfull{m2l} operation, a critical bottleneck for \acrshortpl{fmm} due to its requirement for significant data re-organisation due to the implicit non-contiguous memory accesses required by this operation. As the \acrshort{m2l} is of convolution type current state of the art approaches are based on \acrfullpl{fft} with an $\bigO{N \log{N}}$ complexity bound, however they require careful explicit vector programming to achieve practical performance. We find that remarkably we are able to construct a highly competitive scheme based on direct matrix compression techniques and \acrfull{blas} operations due to the superior memory re-use of the approach, despite it requiring a greater number of \acrshortpl{flop}. Indeed, our scheme's reliance on \acrshort{blas} operations, and comparatively simple algorithmic structure and implementation, make it well suited to direction of development of computer hardware, and depending on the underlying hardware can even offer faster performance than the current state of the art approach. The remainder of the chapter describes the formulation of the other operators from which the \acrshort{kifmm} is composed in both a single node, and present our approach for how these operators are extended to a distributed setting, and the simplified communication reducing strategies we use.

The requirements of software engineering in science, with a focus on rapid iteration, small teams, and requirements for high performance, make the selection of an appropriate programming environment crucial to the success of scientific software projects. In Chapter \ref{chpt:programming_for_science} we present an investigation on scientific programming environments. Discussing the trade-off between high and low-level languages. Specifically our language of choice Rust, a modern systems-level programming language rapidly growing in popularity in science and engineering, is evaluated with respect to our previous approach that used Python, the de-facto high-level language of choice for high-level scientific computing and radiply developing features that enable high-performance.

Chapter \ref{chpt:software_design} describes in detail the engineering approach of our software, particularly the employment of Rust's `trait' system for the implementation of `data oriented design', which is a software design philosophy that emphasises that business logic should minimise memory movements, and that data structures should be as close to contiguously layed out as possible. We present the utility of our design via a set of case studies that demonstrate its flexibility. In particular we examine how traits enable a flexible approach for switching between differing implementations of operators (Section \ref{chpt:software_design:sec:m2l}), single and multi node settings (Section \ref{chpt:software_design:sec:trees}) and indeed different algorithms for the \acrshort{fmm} (Section \ref{chpt:software_design:sec:metadata}).

Chapter \ref{chpt:experiments} contains benchmark experiments for our software, on a single node for Laplace and Helmholtz problems as well as in a \acrshort{hpc} setting for Laplace problems, for which we find we are able to scale our software to the order of $X$ unknowns. Additionally, though the \acrshort{kifmm} is presented for non-oscillatory problems, or oscillatory problems at `low' frequencies, we examine in Section \ref{chpt:experiments:sec:helmholtz:sub:high_k} just how high frequencies can be taken, given a highly performant \acrshort{kifmm} implementation, finding that we are able to compute even highly-oscillatory problems in practically useful times, despite losing out on optimal scaling presented in other methods. We conclude with a reflection on our results and suggestions for future avenues of investigation in Chapter \ref{chpt:conclusion}.


