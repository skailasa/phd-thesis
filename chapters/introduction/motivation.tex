\section{Motivation}\label{chpt:introduction:sec:motivation}




Since its introduction in the late 1980s by Greengard and Rokhlin \cite{greengard1987fast}, the \acrfull{fmm} has become a hallmark algorithm of scientific computing often cited as one of the `top 10' algorithmic advances of the past century \cite{cipra2000best}. The problem it addresses was originally motivated by $N$ particle simulations in which the interactions are \textit{global} but with a strongly decaying property. Motivating examples being $N$ particles interacting via gravity or electrostatic forces. In such cases and for interactions delineated by particular interaction \textit{kernels}, interactions between distant \textit{clusters} of particles can be represented by truncated series expansions. This is indeed where the name for the original presentation originated, as multipole expansions derived from the fundamental solution of the Poisson equation, often called the \textit{Laplace kernel} in \acrshort{fmm} literature were used to form these truncated series expansions,


\begin{equation}
    K(\mathbf{x-y}) =  \begin{cases}
        &= -\frac{1}{2\pi} \log(|\mathbf{x-y}|) \text{, $d$ = 2} \\
        &= \frac{1}{4\pi | \mathbf{x-y}|} \text{, $d$=3 }
    \end{cases}
    \label{eq:chpt:introduction:sec:motivation:laplace_kernel}
\end{equation}


where $d$ is the spatial dimension. Furthermore, by using a hierarchical discretisation for the problem domain, increasingly distant interactions can be captured while still using truncated sums to express the potential due to particles contained within each subdomain in the hierarchy. With this, the \acrshort{fmm} is able to reduce the $\bigO{N^2}$ operations required to evaluate these $N$ problem into an algorithm requiring just $\bigO{N}$ for problems described by the Laplace kernel (\ref{eq:chpt:introduction:sec:motivation:laplace_kernel}). The crucial advantage of the \acrshort{fmm} is that it comes equipped with rigorous error estimates, which guarantee exponential convergence with increasing numbers of series terms used in the truncated expansion, such that the problem could be evaluated to any desired accuracy while retaining the $\bigO{N}$ complexity bound for number of operations.

Despite being a well established algorithm, with numerous variants [CITE VARIANTS] and software efforts [CITE software] over the preceding decades, unlocking the highest available performance for practical \acrshort{fmm} implementations remains an active area of research. Principally this can be attributed to the dramatic changes in the landscape of computing technologies in the decades since the algorithm's first introduction.

Since the end of Dennard Scaling\footnote{First articulated in 1974 by Robert Dennard, Dennard scaling described how as transistors were miniaturised their power density remarkably was able to be maintained as a constant. This held true until the mid 2000s, at which point physical limits on heat dissipation and leakage current lead to power efficiency gains via miniaturisation plateauing despite the steady increase in miniaturisation described by Moore's law, marking the end of Dennard scaling. This in turn lead to the growth of multicore processors and specialised hardware accelerators, as a way to increase available computing performance without increasing power consumption.} in the mid 2000s, hardware design and development has focussed on enhancing parallelism


Heterogenous computing \dots

The direct evaluations of (\ref{eq:chpt:introduction:sec:motivation:laplace_kernel}) can be seen to be embarrassingly parallel over each target particle, and are therefore well represented in hardware by the \acrshort{simd} and \acrshort{simt} paradigms. Remaining open questions are about how heterogenous hardware, modern runtimes which can break up and optimally schedule subroutines of the \acrshort{fmm}, and memory bound subroutines can be optimally designed and deployed so as to reflect hardware developments are


Speed for speed's sake, despite being an interesting proposition from a computational point of view, isn't without application.

- Why might faster simulations be helpful?
    - Unlock more detailed scientific simulations
    - Establish norms for implementation that reflects the capabilities of new hardwares

Indeed, faster particle simulations, of which the \acrshort{fmm} is an example, have been identified as a key operation for optimisation for the exascale era [BERKELEY seven dwarf] due to their broad utility across scientific computing.


Software efforts for \acrshort{fmm}s were a particular focus of activity in the 2010s, with prominent examples being the ExaFMM project [CITATION], ScalFMM [CITATION] and PVFMM [CITATION]. Indeed, \acrshort{fmm} software have cumulatively been the recipients of numerous Gordon Bell awards in the past decade [CITATIONS]. The collective weakness of existing software efforts however is their brittleness, due to the challenges of achieving high performance in a distributed memory setting and for three dimensional problems software, largely developed in a research setting, are rarely maintained with little documentation beyond published performance metrics, and optimisations designed to take advantage of or showcase a particular hardware or algorithmic approach. Therefore an additional focus of this thesis is the development of a \textit{platform} for developing \acrshort{fmm}s and its variants, for which much of the underlying machinery can be re-used. Modular design is critical to encouraging open-source contributions, and software that thrives even after the completion of a doctoral research project or a grant. A dual emphasis both on performance and design results in

- Something people can pick up and use on pretty much any device, especially non-specialists
- Something which can be maintained even after a research project has ended.
- Something that is being actively used by the community, resulting in iterative progress.
